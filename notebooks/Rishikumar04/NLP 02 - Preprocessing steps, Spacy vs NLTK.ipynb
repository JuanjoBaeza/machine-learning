{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de54d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Rishikumar04/Natural-Language-Processing/blob/main/03-Preprocessing%20Steps%20-%20spacy%20vs%20nltk.ipynb\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4bdeb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In|computing|,|plain|text|is|a|loose|term|for|data|that|represent|          |only|characters|of|readable|material|but|not|its|graphical|representation|nor|other|objects|.|"
     ]
    }
   ],
   "source": [
    "doc = nlp('In computing, plain text is a loose term for data that represent \\\n",
    "          only characters of readable material but not its graphical representation nor other objects.')\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8785ad66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In computing, plain text is a loose term for data that represent           only characters of readable material but not its graphical representation nor other objects.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9965a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing NLTK\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3de2b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In computing, plain text is a loose term for data that represent           only characters of readable material but not its graphical representation nor other objects.']\n",
      "No of tokens: 1\n"
     ]
    }
   ],
   "source": [
    "sample = 'In computing, plain text is a loose term for data that represent \\\n",
    "          only characters of readable material but not its graphical representation nor other objects.'\n",
    "\n",
    "tokens = nltk.sent_tokenize(sample)\n",
    "\n",
    "print(tokens)\n",
    "print('No of tokens:', len(tokens) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a1b7529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'computing', ',', 'plain', 'text', 'is', 'a', 'loose', 'term', 'for', 'data', 'that', 'represent', 'only', 'characters', 'of', 'readable', 'material', 'but', 'not', 'its', 'graphical', 'representation', 'nor', 'other', 'objects', '.']\n",
      "No of tokens: 27\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sample)\n",
    "\n",
    "print(tokens)\n",
    "print('No of tokens:', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db7a61b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'computing', ',', 'plain', 'text', 'is', 'a', 'loose', 'term', 'for', 'data', 'that', 'represent', 'only', 'characters', 'of', 'readable', 'material', 'but', 'not', 'its', 'graphical', 'representation', 'nor', 'other', 'objects', '.']\n",
      "No of tokens: 27\n"
     ]
    }
   ],
   "source": [
    "tok = nltk.toktok.ToktokTokenizer()\n",
    "tokens = tok.tokenize(sample)\n",
    "\n",
    "print(tokens)\n",
    "print('No of tokens:', len(tokens) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75eda0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "# Import the toolkit and the full Porter Stemmer library\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15be636e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "# Import the Snow Ball Stemmer library\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# The Snowball Stemmer requires that you pass a language parameter\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7b8a0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous --> generous\n",
      "generation --> generat\n",
      "generously --> generous\n",
      "generate --> generat\n"
     ]
    }
   ],
   "source": [
    "words = ['generous','generation','generously','generate']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84af9f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> run\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easy\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "# Import the Lancaster\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+ls.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04c0e083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I          -- -PRON-    \n",
      "am         -- be        \n",
      "a          -- a         \n",
      "runner     -- runner    \n",
      "running    -- run       \n",
      "in         -- in        \n",
      "a          -- a         \n",
      "race       -- race      \n",
      "because    -- because   \n",
      "I          -- -PRON-    \n",
      "love       -- love      \n",
      "to         -- to        \n",
      "run        -- run       \n",
      "since      -- since     \n",
      "I          -- -PRON-    \n",
      "ran        -- run       \n",
      "today      -- today     \n"
     ]
    }
   ],
   "source": [
    "# Import the Snow Ball Ste##Spacy\n",
    "doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran today\")\n",
    "\n",
    "for token in doc1:\n",
    "    print(f'{token.text:{10}} -- {token.lemma_:{10}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79b53eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown fox are quick and they are jumping over the sleeping lazy dog !'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##NLTK lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer() #Creating object of word net lemmatizer\n",
    "text = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "lemmatized_text = ' '.join(wnl.lemmatize(token) for token in tokens)\n",
    "\n",
    "#Nltk's lemmatization method require positional tag to perform well.\n",
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04b09f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('foxes', 'NNS'), ('are', 'VBP'), ('quick', 'JJ'), ('and', 'CC'), ('they', 'PRP'), ('are', 'VBP'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('sleeping', 'VBG'), ('lazy', 'JJ'), ('dogs', 'NNS'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f537aaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    }
   ],
   "source": [
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a272a495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default words: 326\n",
      "After adding: 327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the word to the set of stop words. Use lowercase!\n",
    "print('Default words:',len(nlp.Defaults.stop_words))\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "\n",
    "# Set the stop_word tag on the lexeme\n",
    "nlp.vocab['btw'].is_stop = True\n",
    "\n",
    "print('After adding:',len(nlp.Defaults.stop_words))\n",
    "\n",
    "nlp.vocab['btw'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2437ff45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the word from the set of stop words\n",
    "nlp.Defaults.stop_words.remove('beyond')\n",
    "\n",
    "# Remove the stop_word tag from the lexeme\n",
    "nlp.vocab['beyond'].is_stop = False\n",
    "\n",
    "print(len(nlp.Defaults.stop_words))\n",
    "\n",
    "nlp.vocab['beyond'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8756b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default length: 179\n",
      "After removing a word length: 178\n",
      "After adding a word length: 179\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print('Default length:', len(stopwords))\n",
    "\n",
    "stopwords.remove('the')\n",
    "print('After removing a word length:', len(stopwords))\n",
    "\n",
    "stopwords.append('brown')\n",
    "print('After adding a word length:', len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6437653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
